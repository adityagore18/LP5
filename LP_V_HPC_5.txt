g%%writefile main.cpp
#include<iostream>
#include<omp.h>
using namespace std;
int minValue(int arr[],int n){
  int minval=arr[0];

  #pragma omp parallel for reduction(min:minval)
  for(int i=0;i<n;i++){
    if(arr[i]<minval) minval=arr[i];
  }

  return minval;
}

int maxValue(int arr[],int n){
  int maxValue=arr[0];

  #pragma omp parallel for reduction(max:maxValue)
  for(int i=0;i<n;i++){
    if(arr[i]>maxValue) maxValue=arr[i];
  }

  return maxValue;
}

int sum(int arr[],int n){
  int sum=0;

  #pragma omp parallel for reduction(+:sum)
  for(int i=0;i<n;i++){
    sum+=arr[i];
  }

  return sum;
}

int avg(int arr[],int n){
  return (double)sum(arr,n)/n;
}

int main(){
   int n = 5;                 // Size of array
  int arr[] = {1,2,3,4,5};   // Initialize array with sample values

  // Display computed statistics
  cout << "The minimum value is: " << minValue(arr, n) << '\n';
  cout << "The maximum value is: " << maxValue(arr, n) << '\n';
  cout << "The summation is: " << sum(arr, n) << '\n';
  cout << "The average is: " << avg(arr, n) << '\n';


  return 0;

}


!g++ -fopenmp main.cpp

!./a.out





"""
# prompt: how exactly above code work how parllel ism happen

The provided C++ code uses OpenMP to parallelize the computation of minimum, maximum, sum, and average of an array. Let's break down how the parallelization works, focusing on the `#pragma omp parallel for` directive and the `reduction` clause.

**1. `#pragma omp parallel for`:**

This directive instructs the OpenMP compiler to parallelize the subsequent loop (the `for` loop in each function).  Instead of executing the loop iterations sequentially, multiple threads will execute different iterations concurrently. The number of threads used is typically determined by the environment variable `OMP_NUM_THREADS`, or a runtime function call, but defaults to the number of cores available on the system.

**2. `reduction(operator:variable)`:**

The `reduction` clause is crucial for correctly handling shared variables within a parallel loop.  Without it, multiple threads would try to update the same variable (`minval`, `maxValue`, or `sum`) simultaneously, leading to race conditions and incorrect results.  The `reduction` clause ensures that each thread operates on a private copy of the variable and then, at the end of the parallel region, the results are combined using the specified `operator`.

* **`reduction(min:minval)` in `minValue()`:** Each thread maintains its own private copy of `minval`.  As it iterates, it updates its private copy if a smaller element is found. After all threads finish, the overall minimum is determined by comparing the private copies, using the `min` operator.
* **`reduction(max:maxValue)` in `maxValue()`:** Similar to `minValue()`, each thread has a private copy of `maxValue`. The final `maxValue` is the maximum among all the private copies.
* **`reduction(+:sum)` in `sum()`:** Each thread computes a partial sum of its assigned elements. The `reduction(+:sum)` combines these partial sums using addition (`+`) to calculate the final total sum.

**How Parallelism Happens (Example with `sum()`):**

1. **Thread Creation:**  OpenMP creates a team of threads.
2. **Loop Distribution:** The `for` loop iterations are divided among these threads.  For instance, if there are 4 threads and 100 iterations, each thread might process roughly 25 iterations.
3. **Private `sum` Copies:** Each thread has its own private variable `sum` initialized to 0.
4. **Partial Summation:**  Each thread independently adds elements of the array segment it has been assigned to its private `sum`.
5. **Reduction Operation:**  When a thread completes its assigned iterations, the private `sum`s are combined using the `+` operator (e.g. 25 + 25 + 25 + 25 for our example). The final `sum` is then the sum of all these private sums.

**In summary:** OpenMP uses multiple threads to execute different parts of the loop simultaneously. The `reduction` clause synchronizes access to shared variables, preventing data corruption, and ensures that the final value of the shared variable correctly reflects the combined results of all the threads.  The threads run in parallel on multiple cores of the processor, significantly speeding up the computation for larger arrays.


The provided code compiles and executes correctly because it uses OpenMP, and the necessary include and compiler flags are used.  The output will reflect the minimum, maximum, sum, and average values of the input array, calculated efficiently by leveraging parallelism."""

